<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Cristian Pamparău</title>
        <!-- Favicon-->
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- Core theme CSS (includes Bootstrap)-->
			<!-- CSS
        ================================================== -->

	<link href='http://fonts.googleapis.com/css?family=Open+Sans:400,300,700' rel='stylesheet' type='text/css'><!-- Fontawesome Icon font -->
	<link href="http://www.eed.usv.ro/mintviz/css/font-awesome.min.css" rel="stylesheet"><!-- bootstrap.min -->
	<link href="http://www.eed.usv.ro/mintviz/css/jquery.fancybox.css" rel="stylesheet"><!-- bootstrap.min -->
	<link href="http://www.eed.usv.ro/mintviz/css/bootstrap.min.css" rel="stylesheet"><!-- bootstrap.min -->
	<link href="http://www.eed.usv.ro/mintviz/css/slit-slider.css" rel="stylesheet"><!-- bootstrap.min -->
	<link href="http://www.eed.usv.ro/mintviz/css/animate.css" rel="stylesheet"><!-- Main Stylesheet -->
	<link href="http://www.eed.usv.ro/mintviz/css/main.css" rel="stylesheet">
	<link href="http://www.eed.usv.ro/mintviz/css/jPushMenu.css" media="all" rel="stylesheet" type="text/css">
	<link href="http://www.eed.usv.ro/mintviz/css/hover.css" media="all" rel="stylesheet" type="text/css">
	<link href="http://www.eed.usv.ro/mintviz/css/preload.css" media="all" rel="stylesheet" type="text/css">
	<link href="http://www.eed.usv.ro/mintviz/css/normalize.css" rel="stylesheet">
	<link href="http://www.eed.usv.ro/mintviz/css/media.css" rel="stylesheet"><!--css for Responsive-->
	<!-- Modernizer Script for old Browsers -->

	<script src="http://eed.usv.ro/mintviz/js/modernizr-2.6.2.min.js">
	</script>
        <link href="css/styles.css" rel="stylesheet" />
    </head>
	<link href="http://www.eed.usv.ro/mintviz/css/articles.css" rel="stylesheet">
    <body>
        <div class="d-flex" id="wrapper">
            <!-- Sidebar-->
            <div class="border-end bg-white" id="sidebar-wrapper">
                <div class="sidebar-heading border-bottom bg-light">Personal Webpage</div>
                <div class="list-group list-group-flush">
                    <a class="list-group-item list-group-item-action list-group-item-light p-3" href="index.html">Home</a>
                    <a class="list-group-item list-group-item-action list-group-item-light p-3" href="publications.html">Publications</a>
                    <a class="list-group-item list-group-item-action list-group-item-light p-3" href="teaching.html">Teaching</a>
                    <a class="list-group-item list-group-item-action list-group-item-light p-3" href="cv.pdf">CV</a>
                </div>
            </div>
            <!-- Page content wrapper-->
            <div id="page-content-wrapper">
                <!-- Top navigation-->
                <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
                    <div class="container-fluid">
                        <button class="btn btn-primary" id="sidebarToggle">Toggle Menu</button>
                        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
                        <div class="collapse navbar-collapse" id="navbarSupportedContent">
                            <ul class="navbar-nav ms-auto mt-2 mt-lg-0">
                                <li class="nav-item active"><a class="nav-link" href="#!">Home</a></li>
                                <li class="nav-item"><a class="nav-link" href="#!">Link</a></li>
                                <li class="nav-item dropdown">
                                    <a class="nav-link dropdown-toggle" id="navbarDropdown" href="#" role="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Dropdown</a>
                                    <div class="dropdown-menu dropdown-menu-end" aria-labelledby="navbarDropdown">
                                        <a class="dropdown-item" href="#!">Action</a>
                                        <a class="dropdown-item" href="#!">Another action</a>
                                        <div class="dropdown-divider"></div>
                                        <a class="dropdown-item" href="#!">Something else here</a>
                                    </div>
                                </li>
                            </ul>
                        </div>
                    </div>
                </nav>
                <!-- Page content-->
                <div class="container-fluid">
                    <h1 class="mt-4">Publications</h1>
					<table>
						<tr>
							<td>
								<ol start="1">
									<li>
										Alexandru-Ionuț Șiean, <b>Cristian Pamparău</b>, Radu-Daniel Vatavu. (2022).
										<a class = "article-title" href = "http://www.eed.usv.ro/mintviz/publications/2022.IMX.2.pdf" target="_blank">
											<b>Scenario-based Exploration of Integrating Radar Sensing into Everyday Objects for Free-Hand Television Control.</b>
										</a>
										In <i>Proceedings of IMX '22, the ACM International Conference on Interactive Media Experiences</i> (Aveiro, JB, Portugal). ACM, New York, NY, USA, 6 pages
										<blockquote class="small" style = "margin-left: 2.5em; margin-bottom: 0.75em; margin-top: 0.25em; margin-right: 3.5em">
											<p class="mb-0 text-justify">
												We address gesture input for TV control, for which we examine mid-air free-hand interactions that can be detected via radar sensing. 
												We adopt a scenario-based design approach to explore possible locations from the living room where to integrate radar sensors, 
												e.g., in the TV set, the couch armrest, or the user’s smartphone, and we contribute a four-level taxonomy of locations relative to the TV
												set, the user, personal robot assistants, and the living room environment, respectively. We also present preliminary results about
												an interactive system using a 15-antenna ultra-wideband 3D radar,
												for which we implemented a dictionary of six directional swipe gestures for the control of dichotomous TV system functions.
											</p>
										</blockquote>
										<a href = "http://www.eed.usv.ro/mintviz/publications/2022.IMX.2.pdf" target="_blank">PDF <i class="fa fa-file-pdf-o" style="font-size: 1.5em;"></i></a> | 
										<a href = "https://doi.org/10.1145/3505284.3532982" target="_blank">DOI <i class="fa fa-external-link-square" style="font-size:1.5em"></i></a> | 
										<!-- <span class = "article-note">ACCEPTANCE RATE: 36.1% (261/722)</span> |
										<span class = "article-note">ARC CORE A*</span> -->
									</li>
								</ol>
							</td>
						</tr>
						<tr>
							<td>
								<ol start="2">
									<li>
										<b>Cristian Pamparău</b>, Radu-Daniel Vatavu. (2022). 
										<a class = "article-title" href = "http://www.eed.usv.ro/mintviz/publications/2022.IMX.1.pdf" target="_blank">
											<b>The User Experience of Journeys in the Realm of Augmented Reality Television.</b>
										</a>
										<i>Proceedings of IMX '22, the ACM International Conference on Interactive Media Experiences</i> (Aveiro, JB, Portugal). ACM, New York, NY, USA, 13 pages
										<blockquote class="small" style = "margin-left: 2.5em; margin-bottom: 0.75em; margin-top: 0.25em; margin-right: 3.5em">
											<p class="mb-0 text-justify">
												Augmented Reality Television (ARTV) can take many forms, from AR content displayed outside the TV frame to video-projected TV
												screens to social TV watching in VR to immersive holograms in the living room. While the user experience (UX) of individual forms
												of ARTV has been documented before, “journeys” as transitions between such forms have not. In this work, we examine the UX of
												watching TV when switching between various levels of augmentation. Our findings from an experiment with fourteen participants 
												reveal an UX characterized by high perceived usability, captivation, and involvement with a low to medium workload and a moderate
												feeling of dissociation from the physical world. We interpret our results in the context of Garrett’s established five-plane model of
												UX—strategy, scope, structure, skeleton, and surface—and propose a sixth plane, “switch,” which separates conceptually the design of 
												user journeys in ARTV from the specifics of the other UX planes.
											</p>
										</blockquote>
										<a href = "http://www.eed.usv.ro/mintviz/publications/2022.IMX.1.pdf" target="_blank">PDF <i class="fa fa-file-pdf-o" style="font-size: 1.5em;"></i></a> | 
										<a href = "https://doi.org/10.1145/3505284.3529969" target="_blank">DOI <i class="fa fa-external-link-square" style="font-size:1.5em"></i></a> | 
										<!-- <span class = "article-note">ACCEPTANCE RATE: 36.1% (261/722)</span> |
										<span class = "article-note">ARC CORE A*</span> -->
									</li>
								</ol>
							</td>
						</tr>
						<tr>
							<td>
								<ol start="3">
									<li>
										<b>Cristian Pamparău</b>, Andrei Costea, Răzvan Jurchiș, Radu-Daniel Vatavu, Adrian Opre. (2022). 
										<a class = "article-title" href = "http://www.eed.usv.ro/mintviz/publications/2022.DAS.pdf" target="_blank">
											<b>Experimental Evaluation of Implicit and Explicit Learning of Abstract Regularities Following Socio-Emotional Interactions in Mixed Reality. </b>
										</a>
										<i>Proceedings of DAS'22, the 16th International Conference on Development and Application Systems</i>. IEEE, Washington, D.C., USA, 150-154
										<blockquote class="small" style = "margin-left: 2.5em; margin-bottom: 0.75em; margin-top: 0.25em; margin-right: 3.5em">
											<p class="mb-0 text-justify">
												We present results from a controlled experiment with N=47 participants conducted in a mixed reality environment
												to assess explicit and implicit learning of cognitive structures instantiated by socio-emotional components. To this end, we
												implemented a custom version of MR4ISL, the Mixed Reality software tool for Implicit Social Learning, with a task involving
												colors, numbers, and emotions. Our results show evidence of explicit learning with participants’ responses being attributed to
												conscious response bases, rules, and memory.
											</p>
										</blockquote>
										<a href = "http://www.eed.usv.ro/mintviz/publications/2022.DAS.pdf" target="_blank">PDF <i class="fa fa-file-pdf-o" style="font-size: 1.5em;"></i></a> | 
										<a href = "https://doi.org/10.1109/DAS54948.2022.9786218" target="_blank">DOI <i class="fa fa-external-link-square" style="font-size:1.5em"></i></a> | 
										WOS: FORTHCOMING
									</li>
								</ol>
							</td>
						</tr>
						<tr>
							<td>
								<ol start="4">
									<li>
										<b>Cristian Pamparău</b>, Radu-Daniel Vatavu, Andrei Costea, Răzvan Jurchiș, Adrian Opre. (2021). 
										<a class = "article-title" href = "http://www.eed.usv.ro/mintviz/publications/2021.MUM.2.pdf" target="_blank">
											<b>XR4ISL: Enabling Psychology Experiments in Extended Reality for Studying the Phenomenon of Implicit Social Learning.</b>
										</a>
										<i>Proceedings of MUM'21, the 20th International Conference on Mobile and Ubiquitous Multimedia</i> (Leuven, Belgium). ACM, New York, NY, USA, 195-197
										<blockquote class="small" style = "margin-left: 2.5em; margin-bottom: 0.75em; margin-top: 0.25em; margin-right: 3.5em">
											<p class="mb-0 text-justify">
												We present XR4ISL, an XR system designed to support psychology experiments examining Implicit Social Learning, 
												a fundamental phenomenon that guides human behavior, cognition, and emotion. 
												We discuss XR4ISL with reference to MR4ISL, a previous system designed for Mixed Reality only, 
												and reflect on differences between Mixed and Virtual Reality for psychology experiments.
											</p>
										</blockquote>
										<a href = "http://www.eed.usv.ro/mintviz/publications/2021.MUM.2.pdf" target="_blank">PDF <i class="fa fa-file-pdf-o" style="font-size: 1.5em;"></i></a> | 
										<a href="https://doi.org/10.1145/3490632.3497830" target="_blank">DOI <i class="fa fa-external-link-square" style="font-size:1.5em"></i></a> |
										<span class = "article-note">ARC B (CORE 2021)</span>
									</li>
								</ol>
							</td>
						</tr>
						<tr>
							<td>
								<ol start="5">
									<li>
										Adrian Aiordachioae, <b>Cristian Pamparau</b>, Radu-Daniel Vatavu. (2021). 
										<a class = "article-title" href = "http://www.eed.usv.ro/mintviz/publications/2022.MTAP.2.pdf" target="_blank">
											<b>Lifelogging Meets Alternate and Cross-Realities: An Investigation into Broadcasting Personal Visual Realities to Remote Audiences.</b>
										</a>
										<i>Multimedia Tools and Applications</i>, 24 pages. Springer
										<blockquote class="small" style = "margin-left: 2.5em; margin-bottom: 0.75em; margin-top: 0.25em; margin-right: 3.5em">
											<p class="mb-0 text-justify">
												We address in this work the topic of broadcasting one’s visual reality,
												captured by the video cameras embedded in mobile and wearable devices, to
												a remote audience. We discuss several designs of such life broadcasting systems, which we position at the intersection of lifelogging, Alternate Reality,
												and Cross-Reality technology. To this end, we introduce the “Alternate Reality
												Broadcast-Time” matrix for the broadcasting and consumption of alternate realities, in which designs of systems that implement sharing and consumption of
												personal visual realities can be positioned, characterized, and compared. These
												design options range from simple video streaming over the web using conventional video protocols to mediated and augmented reality, to audio narration and
												vibrotactile rendering of concepts automatically detected from video captured
												by wearable cameras. To demonstrate the usefulness of our broadcast-time matrix, we describe three prototypes implementing lifelogging, concept recognition from video, augmented and mediated vision, and vibrotactile feedback. Our
												contributions open the way toward new applications that blend lifelogging, consumption of multimedia alternate realities, and XR technology to empower users
												with richer opportunities for self-expression and new means to connect with the
												followers of events in their lives.
											</p>
										</blockquote>
										<a href = "http://www.eed.usv.ro/mintviz/publications/2022.MTAP.2.pdf" target="_blank">PDF <i class="fa fa-file-pdf-o" style="font-size: 1.5em;"></i></a> | 
										<a href="https://doi.org/10.1007/s11042-021-11310-3" target="_blank">DOI <i class="fa fa-external-link-square" style="font-size:1.5em"></i></a> |
										<span class = "article-note">IF: 2.757, 5-Year IF: 2.517 (JCR 2020)</span>
									</li>
								</ol>
							</td>
						</tr>
						<tr>
							<td>
								<ol start="6">
									<li>
										<b>Cristian Pamparău</b>, Radu-Daniel Vatavu, Andrei Costea, Răzvan Jurchiș, Adrian Opre. (2021). 
										<a class = "article-title" href = "http://www.eed.usv.ro/mintviz/publications/2021.EICS.pdf" target="_blank">
											<b>MR4ISL: A Mixed Reality System for Psychological Experiments Focused on Social Learning and Social Interactions.</b>
										</a>
										<i>Companion of the 2021 ACM SIGCHI Symposium on Engineering Interactive Computing Systems</i> (Virtual Event, Eindhoven, Netherlands). ACM, New York, NY, USA, 26-31
										<blockquote class="small" style = "margin-left: 2.5em; margin-bottom: 0.75em; margin-top: 0.25em; margin-right: 3.5em">
											<p class="mb-0 text-justify">
												We present MR4ISL (Mixed Reality for Implicit Social Learning), a HoloLens application designed to examine the psychological aspects
												involved by implicit social learning, a key process responsible for information acquisition at an unconscious level that influences
												humans' behavioral, cognitive, and emotional functioning. We describe the engineering details of MR4ISL, present our roadmap for 
												identifying technical solutions for believable animations of virtual avatars relevant for implicit social learning, and exemplify use cases 
												of MR4ISL that emerged from discussions with three researchers from psychology. To date, MR4ISL is the only tool that uses mixed 
												reality simulations to increase the external validity of psychological research in the study of implicit social learning.
											</p>
										</blockquote>
										<a href = "http://www.eed.usv.ro/mintviz/publications/2021.EICS.pdf" target="_blank">PDF <i class="fa fa-file-pdf-o" style="font-size: 1.5em;"></i></a> | 
										<a href="https://doi.org/10.1145/3459926.3464762" target="_blank">DOI <i class="fa fa-external-link-square" style="font-size:1.5em"></i></a>
									</li>
								</ol>
							</td>
						</tr>
												<tr>
							<td>
								<ol start="7">
									<li>
										<b>Cristian Pamparau</b>, Radu-Daniel Vatavu. (2021). 
										<a class = "article-title" href = "https://doi.org/10.1007/s11042-020-10164-5" target="_blank">
											<b>FlexiSee: Flexible Configuration, Customization, and Control of Mediated and Augmented Vision for Users of Smart Eyewear Devices.</b>
										</a>
										<i>Multimedia Tools and Applications</i> 80, 30943–30968. Springer
										<blockquote class="small" style = "margin-left: 2.5em; margin-bottom: 0.75em; margin-top: 0.25em; margin-right: 3.5em">
											<p class="mb-0 text-justify">
												Smart eyewear and Augmented Reality technology have been examined closely in the scientific community to provide vision rehabilitation to people
												with visual impairments as well as augmented vision to people with and without
												visual impairments for various application scenarios and contexts of use. However, current systems lack flexibility in the configuration and customization of
												the features and functionalities they present to their users, as we show in this
												paper by means of a thorough literature review and categorization of prior work
												on augmented and mediated vision for smart eyewear devices. To address the
												flexibility aspect that has been missing in prior work, we introduce FlexiSee,
												an application for smart eyewear devices, such as see-through Augmented Reality glasses and Head-Mounted Mixed Reality Displays, specifically designed to
												enable flexible configuration, customization, and control of both augmented and
												mediated vision. FlexiSee achieves this desiderata by implementing visual filters
												(e.g., color correction, edge highlighting, contrast adjustment, and others) that
												are coupled with a web-based interface, readily accessible from smartphones,
												tablets, smartwatches, and other devices with web browsers, where authorized
												users can specify and apply custom parameters for the visual filters implemented
												by FlexiSee. We also introduce FlexiSee-DS, a three-dimensional design space
												for FlexiSee-like applications, that includes mediation & augmentation, user categories, and control design dimensions to specify a variety of FlexiSee-like systems. We show how the dimensions of FlexiSee-DS were applied to inform the
												design of our FlexiSee system, and we highlight and focus on the distinction
												between primary users and vision monitors and assistants, where the latter two
												categories represent new types of users for augmented and mediated vision that
												have various degrees of control, from their remote locations, over the visual reality delivered to and perceived by the primary users of smart eyewear devices.
												We conduct a user study to understand the perception of vision monitors and
												assistants regarding our new FlexiSee concept and system, and we report empirical results about usability aspects (e.g., we found an average SUS score of
												75.3 and high ratings for the perceived usefulness of FlexiSee) as well as user
												feedback and suggestions to inform further developments of FlexiSee-like systems and applications.
											</p>
										</blockquote>
										<a href = "http://www.eed.usv.ro/mintviz/publications/2021.MTAP.1.pdf" target="_blank">PDF <i class="fa fa-file-pdf-o" style="font-size: 1.5em;"></i></a> | 
										<a href="https://doi.org/10.1007/s11042-020-10164-5" target="_blank">DOI <i class="fa fa-external-link-square" style="font-size:1.5em"></i></a>
										<span class = "article-note">IF: 2.757, 5-Year IF: 2.517 (JCR 2020)</span> |
										<span class = "article-note">WOS: 000604203000002<span> 
									</li>
								</ol>
							</td>
						</tr>
						<tr>
							<td>
								<ol start="8">
									<li>
										<b>Cristian Pamparău</b>, Radu-Daniel Vatavu. (2020). 
										<a class = "article-title" href = "http://www.eed.usv.ro/mintviz/publications/2020.MUM.4.pdf" target="_blank">
											<b>A Research Agenda Is Needed for Designing for the User Experience of Augmented and Mixed Reality: A Position Paper.</b>
										</a>
										<i>Proceedings of MUM '20, the 19th ACM International Conference on Mobile and Ubiquitous Multimedia</i> (Virtual Event, Essen, Germany). ACM, New York, NY, USA, 323-325
										<blockquote class="small" style = "margin-left: 2.5em; margin-bottom: 0.75em; margin-top: 0.25em; margin-right: 3.5em">
											<p class="mb-0 text-justify">
												With this position paper, we want to draw the attention of the community toward theoretical work and practical opportunities
												that have been overlooked so far regarding concepts, principles, and design knowledge for the user experience of Augmented and
												Mixed Reality content, I/O devices, interactions, applications, and systems. Despite considerable innovations in commercial products
												and research prototypes enabling Augmented and Mixed Reality worlds, how to design great user experiences in such worlds has
												been overall neglected at core, while the information and knowledge currently available to practitioners cover usability aspects mostly. 
												Therefore, we advocate for theoretical foundations of the user experience in Augmented and Mixed Reality and propose several directions for 
												more scientific research in this regard.
											</p>
										</blockquote>
										<a href = "http://www.eed.usv.ro/mintviz/publications/2020.MUM.4.pdf" target="_blank">PDF <i class="fa fa-file-pdf-o" style="font-size: 1.5em;"></i></a> | 
										<a href="https://doi.org/10.1145/3428361.3432088" target="_blank">DOI <i class="fa fa-external-link-square" style="font-size:1.5em"></i></a> |
										<span class = "article-note">ARC B (CORE 2021) </span>
									</li>
								</ol>
							</td>
						</tr>
						<tr>
							<td>
								<ol start="9">
									<li>
										<b>Cristian Pamparău</b>, Adrian Aiordăchioae, Radu-Daniel Vatavu. (2020). 
										<a class = "article-title" href = "https://doi.org/10.1145/3428361.3432089" target="_blank">
											<b>From Do You See What I See? to Do You Control What I See? Mediated Vision, From a Distance, for Eyewear Users</b>
										</a>
										<i>Proceedings of MUM '20, the 19th ACM International Conference on Mobile and Ubiquitous Multimedia</i> (Virtual Event, Essen, Germany). ACM, New York, NY, USA, 323-325
										<blockquote class="small" style = "margin-left: 2.5em; margin-bottom: 0.75em; margin-top: 0.25em; margin-right: 3.5em">
											<p class="mb-0 text-justify">
												We discuss engineering aspects for shifting from “do you see what
												I see?” applications that stream the user’s field of view to remote
												viewers toward “do you control what I see?” features in which remote
												viewers are given the opportunity and tool to control the primary
												user’s field of view. To this end, we present two applications for (1)
												smartglasses with embedded video camera for live video streaming
												and (2) the HoloLens HMD that presents users with mediated
												versions of the visual world controlled by remote viewers.
											</p>
										</blockquote>
										<a href = "https://doi.org/10.1145/3428361.3432089" target="_blank">PDF <i class="fa fa-file-pdf-o" style="font-size: 1.5em;"></i></a> | 
										<a href="https://doi.org/10.1145/3428361.3432089" target="_blank">DOI <i class="fa fa-external-link-square" style="font-size:1.5em"></i></a> |
										<span class = "article-note">ARC B (CORE 2021) </span>
									</li>
								</ol>
							</td>
						</tr>
						<tr>
							<td>
								<ol start="10">
									<li>
										<b>Cristian Pamparău</b> (2020). 
										<a class = "article-title" href = "http://www.dasconference.ro/cd2020/data/papers/D35-paper.pdf" target = "_blank">
											<b>A System for Hierarchical Browsing of Mixed Reality Content in Smart Spaces.</b>
										</a>
										<i>Proceedings of DAS 2020, the 15th International Conference on Development and Application Systems</i> (Suceava, Romania).
										IEEE, 194-197
										<blockquote class="small" style = "margin-left: 2.5em; margin-bottom: 0.75em; margin-top: 0.25em; margin-right: 3.5em">
											<p class="mb-0 text-justify">
												We present in this paper technical and engineering details of a wearable system for visualizing digital content. 
												The digital content is overlaid onto the physical space by employing hierarchical structures for content organization. 
												We implement analogies with the physical world adapting laws of physics, such as gravity, to digital content. To this end, 
												we rely on concepts from the fields of Augmented Reality, Wearable Computing, Software Engineering, 
												and Human-Computer Interaction to propose a new model for accessing digital content anchored in the physical reality. 
												In our prototype, reality is augmented with containers of digital content represented graphically as semiopaque spheres 
												that float in the physical space around the user. We describe the properties of the spherical containers of 
												digital content, such as their size and relative location in the physical environment, and their relationships to 
												the user and the physical world. We also present interaction techniques based on gesture input that enable users to 
												access the digital content contained by the virtual spheres. We present our technical implementation using the 
												Microsoft holographic computer HoloLens and exemplify use case scenarios for our wearable prototype.
											</p>
										</blockquote>
										<br>
										<a href = "http://www.dasconference.ro/cd2020/data/papers/D35-paper.pdf" target="_blank">PDF <i class="fa fa-file-pdf-o" style="font-size: 1.5em;"></i></a> |
										<a href="https://ieeexplore.ieee.org/document/9108977" target="_blank">DOI <i class="fa fa-external-link-square" style="font-size:1.5em;"></i></a> | 
									</li>
								</ol>
							</td>
						</tr>
					</table>
                </div>
            </div>
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
